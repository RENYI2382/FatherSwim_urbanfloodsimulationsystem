"""
çœŸå®æ•°æ®é›†æˆæ¨¡å—
ç”¨äºé›†æˆå’Œåˆ†ætsinghua-fib-lab/hurricane-mobility-generation-benchmarkæ•°æ®é›†
åŒ…æ‹¬columbia.pbæ–‡ä»¶åˆ†æå’Œprofiles.jsonæ•°æ®å¤„ç†
"""

import json
import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import pickle
import struct
from dataclasses import dataclass
from collections import defaultdict

logger = logging.getLogger(__name__)

@dataclass
class RealDataProfile:
    """çœŸå®æ•°æ®ç”¨æˆ·é…ç½®æ–‡ä»¶"""
    id: int
    home: int
    work: int
    gender: str
    race: str
    education: str
    income: int
    consumption: str
    age: int
    
    def get_income_level(self) -> str:
        """æ ¹æ®æ”¶å…¥è·å–æ”¶å…¥ç­‰çº§"""
        if self.income < 35000:
            return "low_income"
        elif self.income > 70000:
            return "high_income"
        else:
            return "middle_income"
    
    def get_age_group(self) -> str:
        """æ ¹æ®å¹´é¾„è·å–å¹´é¾„ç»„"""
        if self.age < 35:
            return "young_adult"
        elif self.age > 65:
            return "elderly"
        else:
            return "middle_age"
    
    def has_vehicle(self) -> bool:
        """æ ¹æ®æ”¶å…¥å’Œå¹´é¾„æ¨æ–­æ˜¯å¦æœ‰è½¦"""
        # åŸºäºç»Ÿè®¡æ•°æ®çš„ç®€å•æ¨æ–­
        if self.age < 18:
            return False
        if self.income > 50000:
            return True
        if self.age > 75:
            return False
        return self.income > 30000

@dataclass
class LocationNode:
    """åœ°ç†ä½ç½®èŠ‚ç‚¹"""
    id: int
    lat: float
    lon: float
    type: str = "unknown"

class RealDataIntegrator:
    """çœŸå®æ•°æ®é›†æˆå™¨"""
    
    def __init__(self, dataset_path: str = None):
        """åˆå§‹åŒ–æ•°æ®é›†æˆå™¨"""
        if dataset_path is None:
            self.dataset_path = Path(".agentsociety-benchmark/datasets/HurricaneMobility")
        else:
            self.dataset_path = Path(dataset_path)
        
        self.profiles_data = None
        self.location_data = None
        self.mobility_patterns = None
        
        logger.info(f"åˆå§‹åŒ–çœŸå®æ•°æ®é›†æˆå™¨ï¼Œæ•°æ®è·¯å¾„: {self.dataset_path}")
    
    def load_profiles(self) -> List[RealDataProfile]:
        """åŠ è½½ç”¨æˆ·é…ç½®æ–‡ä»¶"""
        profiles_file = self.dataset_path / "profiles.json"
        
        if not profiles_file.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {profiles_file}")
        
        logger.info(f"åŠ è½½ç”¨æˆ·é…ç½®æ–‡ä»¶: {profiles_file}")
        
        with open(profiles_file, 'r', encoding='utf-8') as f:
            profiles_raw = json.load(f)
        
        profiles = []
        for profile_data in profiles_raw:
            profile = RealDataProfile(
                id=profile_data['id'],
                home=profile_data['home'],
                work=profile_data['work'],
                gender=profile_data['gender'],
                race=profile_data['race'],
                education=profile_data['education'],
                income=profile_data['income'],
                consumption=profile_data['consumption'],
                age=profile_data['age']
            )
            profiles.append(profile)
        
        self.profiles_data = profiles
        logger.info(f"æˆåŠŸåŠ è½½ {len(profiles)} ä¸ªç”¨æˆ·é…ç½®æ–‡ä»¶")
        return profiles
    
    def analyze_columbia_pb(self) -> Dict[str, Any]:
        """åˆ†æcolumbia.pbæ–‡ä»¶"""
        pb_file = self.dataset_path / "columbia.pb"
        
        if not pb_file.exists():
            raise FileNotFoundError(f"åœ°å›¾æ–‡ä»¶ä¸å­˜åœ¨: {pb_file}")
        
        logger.info(f"åˆ†æåœ°å›¾æ–‡ä»¶: {pb_file}")
        
        try:
            # å°è¯•è¯»å–protobufæ–‡ä»¶
            with open(pb_file, 'rb') as f:
                data = f.read()
            
            # åŸºæœ¬æ–‡ä»¶ä¿¡æ¯
            file_size = len(data)
            
            # å°è¯•è§£æåŸºæœ¬ç»“æ„
            analysis = {
                "file_size": file_size,
                "file_size_mb": round(file_size / (1024 * 1024), 2),
                "data_type": "protobuf",
                "analysis_timestamp": pd.Timestamp.now().isoformat()
            }
            
            # å°è¯•è¯†åˆ«æ•°æ®æ¨¡å¼
            # æŸ¥æ‰¾é‡å¤çš„æ•°æ®æ¨¡å¼ï¼Œå¯èƒ½è¡¨ç¤ºèŠ‚ç‚¹æˆ–è¾¹
            chunk_size = 1000
            patterns = defaultdict(int)
            
            for i in range(0, min(len(data), 100000), chunk_size):
                chunk = data[i:i+chunk_size]
                # æŸ¥æ‰¾å¯èƒ½çš„IDæ¨¡å¼ï¼ˆ4å­—èŠ‚æ•´æ•°ï¼‰
                for j in range(0, len(chunk)-4, 4):
                    try:
                        value = struct.unpack('<I', chunk[j:j+4])[0]
                        if 500000000 <= value <= 600000000:  # åŸºäºprofilesä¸­çš„IDèŒƒå›´
                            patterns[value] += 1
                    except:
                        continue
            
            # ç»Ÿè®¡å¯èƒ½çš„èŠ‚ç‚¹æ•°é‡
            potential_nodes = len([k for k, v in patterns.items() if v >= 2])
            analysis["potential_nodes"] = potential_nodes
            analysis["id_patterns_found"] = len(patterns)
            
            logger.info(f"åœ°å›¾æ–‡ä»¶åˆ†æå®Œæˆ: {analysis['file_size_mb']}MB, æ½œåœ¨èŠ‚ç‚¹æ•°: {potential_nodes}")
            
            return analysis
            
        except Exception as e:
            logger.error(f"åˆ†æcolumbia.pbæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return {
                "error": str(e),
                "file_size": file_size,
                "analysis_timestamp": pd.Timestamp.now().isoformat()
            }
    
    def analyze_demographic_patterns(self) -> Dict[str, Any]:
        """åˆ†æäººå£ç»Ÿè®¡å­¦æ¨¡å¼"""
        if self.profiles_data is None:
            self.load_profiles()
        
        logger.info("åˆ†æäººå£ç»Ÿè®¡å­¦æ¨¡å¼")
        
        df = pd.DataFrame([
            {
                'id': p.id,
                'age': p.age,
                'income': p.income,
                'gender': p.gender,
                'race': p.race,
                'education': p.education,
                'consumption': p.consumption,
                'age_group': p.get_age_group(),
                'income_level': p.get_income_level(),
                'has_vehicle': p.has_vehicle()
            }
            for p in self.profiles_data
        ])
        
        analysis = {
            "total_profiles": len(df),
            "age_distribution": {
                "mean": df['age'].mean(),
                "std": df['age'].std(),
                "min": df['age'].min(),
                "max": df['age'].max(),
                "groups": df['age_group'].value_counts().to_dict()
            },
            "income_distribution": {
                "mean": df['income'].mean(),
                "std": df['income'].std(),
                "min": df['income'].min(),
                "max": df['income'].max(),
                "levels": df['income_level'].value_counts().to_dict()
            },
            "gender_distribution": df['gender'].value_counts().to_dict(),
            "race_distribution": df['race'].value_counts().to_dict(),
            "education_distribution": df['education'].value_counts().to_dict(),
            "consumption_distribution": df['consumption'].value_counts().to_dict(),
            "vehicle_ownership": df['has_vehicle'].value_counts().to_dict()
        }
        
        # è®¡ç®—ç›¸å…³æ€§
        numeric_cols = ['age', 'income']
        correlation_matrix = df[numeric_cols].corr().to_dict()
        analysis["correlations"] = correlation_matrix
        
        logger.info(f"äººå£ç»Ÿè®¡å­¦åˆ†æå®Œæˆï¼Œæ€»è®¡ {len(df)} ä¸ªé…ç½®æ–‡ä»¶")
        
        return analysis
    
    def extract_mobility_patterns(self) -> Dict[str, Any]:
        """æå–ç§»åŠ¨æ¨¡å¼"""
        if self.profiles_data is None:
            self.load_profiles()
        
        logger.info("æå–ç§»åŠ¨æ¨¡å¼")
        
        # åˆ†æå®¶åº­-å·¥ä½œåœ°ç‚¹åˆ†å¸ƒ
        home_locations = [p.home for p in self.profiles_data]
        work_locations = [p.work for p in self.profiles_data]
        
        # è®¡ç®—è·ç¦»åˆ†å¸ƒï¼ˆåŸºäºIDå·®å¼‚çš„ç®€å•ä¼°è®¡ï¼‰
        distances = []
        for p in self.profiles_data:
            # ç®€å•çš„è·ç¦»ä¼°è®¡ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨åœ°ç†åæ ‡ï¼‰
            distance_proxy = abs(p.home - p.work)
            distances.append(distance_proxy)
        
        patterns = {
            "home_work_analysis": {
                "unique_home_locations": len(set(home_locations)),
                "unique_work_locations": len(set(work_locations)),
                "avg_distance_proxy": np.mean(distances),
                "std_distance_proxy": np.std(distances)
            },
            "location_clusters": {
                "home_id_range": {
                    "min": min(home_locations),
                    "max": max(home_locations)
                },
                "work_id_range": {
                    "min": min(work_locations),
                    "max": max(work_locations)
                }
            }
        }
        
        # æŒ‰äººå£ç¾¤ä½“åˆ†æç§»åŠ¨æ¨¡å¼
        by_age_group = defaultdict(list)
        by_income_level = defaultdict(list)
        
        for p in self.profiles_data:
            distance = abs(p.home - p.work)
            by_age_group[p.get_age_group()].append(distance)
            by_income_level[p.get_income_level()].append(distance)
        
        patterns["by_demographics"] = {
            "by_age_group": {
                group: {
                    "count": len(distances),
                    "avg_distance": np.mean(distances),
                    "std_distance": np.std(distances)
                }
                for group, distances in by_age_group.items()
            },
            "by_income_level": {
                level: {
                    "count": len(distances),
                    "avg_distance": np.mean(distances),
                    "std_distance": np.std(distances)
                }
                for level, distances in by_income_level.items()
            }
        }
        
        self.mobility_patterns = patterns
        logger.info("ç§»åŠ¨æ¨¡å¼æå–å®Œæˆ")
        
        return patterns
    
    def generate_calibration_data(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ ¡å‡†æ•°æ®"""
        if self.profiles_data is None:
            self.load_profiles()
        
        logger.info("ç”Ÿæˆæ ¡å‡†æ•°æ®")
        
        # åŸºäºçœŸå®æ•°æ®ç”Ÿæˆæ ¡å‡†å‚æ•°
        demographic_analysis = self.analyze_demographic_patterns()
        mobility_patterns = self.extract_mobility_patterns()
        
        # ç”Ÿæˆå¹´é¾„ç»„çš„ç§»åŠ¨å€æ•°
        age_multipliers = {}
        age_groups = demographic_analysis["age_distribution"]["groups"]
        
        for group, count in age_groups.items():
            if group == "young_adult":
                # å¹´è½»äººæ›´æ´»è·ƒ
                base_multiplier = 1.2
            elif group == "elderly":
                # è€å¹´äººè¾ƒä¿å®ˆ
                base_multiplier = 0.8
            else:
                # ä¸­å¹´äººé€‚ä¸­
                base_multiplier = 1.0
            
            age_multipliers[group] = {
                "base_multiplier": base_multiplier,
                "population_ratio": count / demographic_analysis["total_profiles"],
                "sample_size": count
            }
        
        # ç”Ÿæˆæ”¶å…¥ç»„çš„ç§»åŠ¨å€æ•°
        income_multipliers = {}
        income_levels = demographic_analysis["income_distribution"]["levels"]
        
        for level, count in income_levels.items():
            if level == "high_income":
                base_multiplier = 1.3
            elif level == "low_income":
                base_multiplier = 0.7
            else:
                base_multiplier = 1.0
            
            income_multipliers[level] = {
                "base_multiplier": base_multiplier,
                "population_ratio": count / demographic_analysis["total_profiles"],
                "sample_size": count
            }
        
        calibration_data = {
            "metadata": {
                "source": "tsinghua-fib-lab/hurricane-mobility-generation-benchmark",
                "total_profiles": demographic_analysis["total_profiles"],
                "generation_timestamp": pd.Timestamp.now().isoformat()
            },
            "demographic_multipliers": {
                "age_groups": age_multipliers,
                "income_levels": income_multipliers
            },
            "baseline_patterns": {
                "vehicle_ownership_rate": demographic_analysis.get("vehicle_ownership", {}).get(True, 0) / demographic_analysis["total_profiles"],
                "avg_age": demographic_analysis["age_distribution"]["mean"],
                "avg_income": demographic_analysis["income_distribution"]["mean"]
            },
            "mobility_characteristics": mobility_patterns
        }
        
        logger.info("æ ¡å‡†æ•°æ®ç”Ÿæˆå®Œæˆ")
        
        return calibration_data
    
    def save_analysis_results(self, output_dir: str = "results") -> str:
        """ä¿å­˜åˆ†æç»“æœ"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # æ‰§è¡Œæ‰€æœ‰åˆ†æ
        pb_analysis = self.analyze_columbia_pb()
        demographic_analysis = self.analyze_demographic_patterns()
        mobility_patterns = self.extract_mobility_patterns()
        calibration_data = self.generate_calibration_data()
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        complete_analysis = {
            "columbia_pb_analysis": pb_analysis,
            "demographic_analysis": demographic_analysis,
            "mobility_patterns": mobility_patterns,
            "calibration_data": calibration_data,
            "analysis_metadata": {
                "dataset_path": str(self.dataset_path),
                "analysis_timestamp": pd.Timestamp.now().isoformat(),
                "version": "1.0.0"
            }
        }
        
        # ä¿å­˜ç»“æœ
        output_file = output_path / "real_data_integration_analysis.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(complete_analysis, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        return str(output_file)

def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•æ•°æ®é›†æˆåŠŸèƒ½"""
    logging.basicConfig(level=logging.INFO)
    
    try:
        # åˆå§‹åŒ–æ•°æ®é›†æˆå™¨
        integrator = RealDataIntegrator()
        
        # æ‰§è¡Œå®Œæ•´åˆ†æ
        result_file = integrator.save_analysis_results()
        
        print(f"âœ… çœŸå®æ•°æ®é›†æˆåˆ†æå®Œæˆ")
        print(f"ğŸ“Š ç»“æœæ–‡ä»¶: {result_file}")
        
        # æ˜¾ç¤ºåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        if integrator.profiles_data:
            print(f"ğŸ‘¥ ç”¨æˆ·é…ç½®æ–‡ä»¶æ•°é‡: {len(integrator.profiles_data)}")
            
        demographic_analysis = integrator.analyze_demographic_patterns()
        print(f"ğŸ“ˆ å¹´é¾„åˆ†å¸ƒ: {demographic_analysis['age_distribution']['groups']}")
        print(f"ğŸ’° æ”¶å…¥åˆ†å¸ƒ: {demographic_analysis['income_distribution']['levels']}")
        
    except Exception as e:
        logger.error(f"æ•°æ®é›†æˆè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        raise

if __name__ == "__main__":
    main()